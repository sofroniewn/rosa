{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "# PATH = '/home/ec2-user/cell_census/tabula_sapiens__sample_single_cell__label_cell_type__processed.h5ad'\n",
    "PATH = '/home/ec2-user/cell_census/tabula_sapiens__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata = ad.read_h5ad(PATH)\n",
    "\n",
    "from enformer_pytorch import GenomeIntervalDataset\n",
    "\n",
    "class MyGenomeIntervalDataset(GenomeIntervalDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyGenomeIntervalDataset, self).__init__(**kwargs)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        item = super().__getitem__(ind)\n",
    "        label = self.df.row(ind)[4]\n",
    "        return label, item\n",
    "\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import zarr\n",
    "from enformer_pytorch import Enformer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pyfaidx\n",
    "from pathlib import Path\n",
    "\n",
    "torch.multiprocessing.freeze_support()\n",
    "\n",
    "BASE_PT = \"/home/ec2-user/enformer\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# BASE_PT = \"/Users/nsofroniew/Documents/data/multiomics/enformer\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "FASTA_PT = BASE_PT + \"/Homo_sapiens.GRCh38.dna.toplevel.fa\"\n",
    "GENE_INTERVALS_PT = BASE_PT + \"/Homo_sapiens.GRCh38.genes.bed\"\n",
    "EMBEDDING_PT = BASE_PT + \"/Homo_sapiens.GRCh38.genes.enformer_embeddings.zarr\"\n",
    "EMBEDDING_PT_TSS = BASE_PT + \"/Homo_sapiens.GRCh38.genes.enformer_embeddings_tss.zarr\"\n",
    "MODEL_PT = \"EleutherAI/enformer-official-rough\"\n",
    "\n",
    "def filter_df_fn(df):\n",
    "    return df.filter(pl.col(\"column_5\").is_in(list(adata.var_names)))\n",
    "\n",
    "print(\"Converting fasta file\")\n",
    "pyfaidx.Faidx(FASTA_PT)\n",
    "print(\"Fasta file done\")\n",
    "\n",
    "model = Enformer.from_pretrained(MODEL_PT, output_heads=dict(human = 5313), use_checkpointing = False)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyGenomeIntervalDataset(\n",
    "    bed_file=GENE_INTERVALS_PT,  # bed file - columns 0, 1, 2 must be <chromosome>, <start position>, <end position>\n",
    "    fasta_file=FASTA_PT,  # path to fasta file\n",
    "    return_seq_indices=False,  # return nucleotide indices (ACGTN) or one hot encodings\n",
    "    rc_aug=False,\n",
    "    filter_df_fn=filter_df_fn,\n",
    ")\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=False, num_workers=0) # type: DataLoader\n",
    "\n",
    "# Create zarr files\n",
    "SEQ_EMBED_DIM = 896\n",
    "EMBED_DIM = 3072\n",
    "NUM_GENES = len(ds)\n",
    "TSS = int(SEQ_EMBED_DIM // 2)\n",
    "\n",
    "paths = (Path(EMBEDDING_PT), Path(EMBEDDING_PT_TSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[181][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# targets_txt = 'https://raw.githubusercontent.com/calico/basenji/0.5/manuscripts/cross2020/targets_human.txt'\n",
    "# df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets = pd.read_csv(BASE_PT + '/targets_human.txt')\n",
    "df_targets.shape  # (5313, 8) With rows match output shape above.\n",
    "cage_indices = np.where(df_targets['description'].str.startswith('CAGE:'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_embedding_full = zarr.open(\n",
    "#     EMBEDDING_PT,\n",
    "#     mode=\"w\",\n",
    "#     shape=(NUM_GENES, SEQ_EMBED_DIM, EMBED_DIM),\n",
    "#     chunks=(1, SEQ_EMBED_DIM, EMBED_DIM),\n",
    "#     dtype='float32',\n",
    "# )\n",
    "\n",
    "# z_embedding_tss = zarr.open(\n",
    "#     EMBEDDING_PT_TSS,\n",
    "#     mode=\"w\",\n",
    "#     shape=(NUM_GENES, EMBED_DIM),\n",
    "#     chunks=(1, EMBED_DIM),\n",
    "#     dtype='float32',\n",
    "# )\n",
    "\n",
    "# index = 0\n",
    "# for labels, batch in tqdm(dl):\n",
    "#     # calculate embedding\n",
    "#     with torch.no_grad():\n",
    "#         output, embeddings = model(batch.to(DEVICE), return_embeddings=True)\n",
    "#         embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "#     tss_embedding = embeddings[:, TSS]\n",
    "\n",
    "#     # save full and reduced embeddings\n",
    "#     batch_size = len(embeddings)\n",
    "#     z_embedding_full[index : index + batch_size] = embeddings\n",
    "#     z_embedding_tss[index : index + batch_size] = tss_embedding\n",
    "#     index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0, :10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0, :10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, embeddings = model(batch.to(DEVICE), return_embeddings=True)\n",
    "    cage_expression = output['human'][:, :, cage_indices].mean(dim=-1)\n",
    "    max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "    batch_size = len(embeddings)\n",
    "    tss_embedding = embeddings[torch.arange(batch_size), max_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings[torch.arange(2), :, max_inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_expression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, seq = ds[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, embeddings = model(seq.to(DEVICE), return_embeddings=True)\n",
    "    cage_expression = output['human'][:, cage_indices].mean(dim=1)\n",
    "    max_ind = torch.argmax(cage_expression)\n",
    "    tss_embedding = embeddings[max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.df['column_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = torch.argmax(cage_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['human'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([TSS, TSS], [0, 100]);\n",
    "plt.plot(output['human'][:, cage_indices].mean(dim=1).detach().cpu());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyensembl import Genome\n",
    "\n",
    "# gtf_file_path = BASE_PT + '/Homo_sapiens.GRCh38.77.gtf'\n",
    "\n",
    "# genome = Genome(reference_name='GRCh38',\n",
    "#     annotation_name='my_genome_features',\n",
    "#     gtf_path_or_url=gtf_file_path)\n",
    "# # genome.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyensembl import EnsemblRelease\n",
    "\n",
    "genome = EnsemblRelease(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = genome.gene_by_id(label)\n",
    "\n",
    "# Observed and predicted gene expression values were obtained by summing up the observed/predicted CAGE read counts\n",
    "# at all unique TSS locations of the gene. For each TSS location, we used the 128-bp bin overlapping the TSS as well\n",
    "# as the two neighboring bins (3 bins in total).\n",
    "\n",
    "# For each gene, look through all transcipts - protein coding / not, and record offsets from gene start\n",
    "gene.transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def get_tss(gene_id, tss=TSS, length=SEQ_EMBED_DIM, sigma=8):\n",
    "    gene = genome.gene_by_id(gene_id)\n",
    "    starts = np.array([tss + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts], dtype=int)\n",
    "    starts = starts[starts>=0]\n",
    "    starts = starts[starts<length]\n",
    "    vector = np.zeros(length)\n",
    "    vector[starts] = 1.0\n",
    "    if sigma is not None:\n",
    "        vector = gaussian_filter1d(vector, sigma)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(get_tss(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tss_locations(genes, tss=TSS):\n",
    "    locations = {}\n",
    "    for gene_id in genes:\n",
    "        gene = genome.gene_by_id(gene_id)\n",
    "        start_diffs = [tss + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts]\n",
    "        locations[gene_id] = start_diffs\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = get_tss_locations(list(ds.df['column_5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = genome.gene_by_id(label)\n",
    "start_diffs = [TSS + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in start_diffs:\n",
    "    plt.plot([st, st], [0, 100], color='k');\n",
    "# plt.plot(output['human'][:, cage_indices].mean(dim=1).cpu());\n",
    "plt.ylim([0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss=TSS):\n",
    "    # Embeddings include\n",
    "    #   TSS\n",
    "    #   sum over all\n",
    "    #   argmax over all\n",
    "    #   sum over TSS\n",
    "    #   argmax over TSS\n",
    "    #   sum over TSS sigma 3, 8, 16\n",
    "    #   argmax over TSS sigma 3, 8, 16\n",
    "    batch_size = embeddings.shape[0]\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    tss_emb = embeddings[:, TSS]\n",
    "    sum_emb = embeddings.sum(dim=1)\n",
    "    max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "    amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "    \n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    amax_tss_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "\n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    sum_tss_emb = (embeddings * scaled_cage_expression).sum(dim=1)\n",
    "\n",
    "    all_emb = [tss_emb, sum_emb, amax_emb, amax_tss_emb, sum_tss_emb]\n",
    "    for sigma in sigmas:\n",
    "        ks = 2 * int(sigma / 2 * 3)\n",
    "        tss_tensors_conv = gaussian_filter_1d(tss_tensors, kernel_size=ks, sigma=sigma)\n",
    "        scaled_cage_expression = cage_expression * tss_tensors_conv\n",
    "        max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "        amax_tss_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "\n",
    "        max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "        sum_tss_emb = (embeddings * scaled_cage_expression).sum(dim=1)\n",
    "\n",
    "        all_emb.append(amax_tss_emb)\n",
    "        all_emb.append(sum_tss_emb)\n",
    "\n",
    "    return torch.stack(all_emb, dim=0) # 5 + 2 * len(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss):\n",
    "    # Embeddings include\n",
    "    #   TSS -1, 0, 1\n",
    "    #   argmax over TSS -1, 0, 1\n",
    "    batch_size = embeddings.shape[0]\n",
    "    len_seq = tss_tensors.shape[1] - 1\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    tss_emb = embeddings[:, tss]\n",
    "    tss_emb_m1 = embeddings[:, tss - 1]\n",
    "    tss_emb_1 = embeddings[:, tss + 1]\n",
    "\n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "    amax_emb_m1 = embeddings[torch.arange(batch_size), torch.clip(max_inds - 1, 0, len_seq)]\n",
    "    amax_emb_1 = embeddings[torch.arange(batch_size), torch.clip(max_inds + 1, 0, len_seq)]\n",
    "    \n",
    "    all_emb = [tss_emb, tss_emb_m1, tss_emb_1, amax_emb, amax_emb_m1, amax_emb_1]\n",
    "    return torch.stack(all_emb, dim=0) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss):\n",
    "    # Embeddings include\n",
    "    #   argmax over TSS -1, 0, 1 for top 5\n",
    "    batch_size = embeddings.shape[0]\n",
    "    len_seq = tss_tensors.shape[1] - 1\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    # max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    topk_inds, tok_values = torch.topk(scaled_cage_expression, 5, dim=-1)\n",
    "    topk_inds[tok_values == 0] = tss\n",
    "\n",
    "    all_emb = []\n",
    "    for i in range(topk_inds.shape[1]):\n",
    "        max_inds = topk_inds[:, i]\n",
    "        amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "        amax_emb_m1 = embeddings[torch.arange(batch_size), torch.clip(max_inds - 1, 0, len_seq)]\n",
    "        amax_emb_1 = embeddings[torch.arange(batch_size), torch.clip(max_inds + 1, 0, len_seq)]\n",
    "        all_emb += [amax_emb, amax_emb_m1, amax_emb_1]\n",
    "    return torch.stack(all_emb, dim=0) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((2, 89))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 49])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(A, 5, dim=-1).indices[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 49])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(A, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_cage_expression = cage_expression * tss_tensors\n",
    "max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "tss_embedding = embeddings[torch.arange(batch_size), max_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "\n",
    "PATH = '/home/ec2-user/cell_census/tabula_sapiens__tss_max__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata = ad.read_h5ad(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AxisArrays with keys: embedding"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata.varm['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "z = zarr.open('/home/ec2-user/enformer/Homo_sapiens.GRCh38.genes.enformer_embeddings_trio_top5_pc_0.zarr', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 19431, 3072)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "#     # Embeddings include\n",
    "#     #   TSS\n",
    "#     #   sum over all\n",
    "#     #   argmax over all\n",
    "#     #   argmax over TSS\n",
    "#     #   sum over TSS\n",
    "#     #   argmax over TSS sigma 3, 8, 16\n",
    "#     #   sum over TSS sigma 3, 8, 16\n",
    "\n",
    "# names = ['', '_sum', '_amax', '_tss_amax', '_tss_sum']\n",
    "# sigmas = [3, 8, 16, 32, 64]\n",
    "# for s in sigmas:\n",
    "#     names.append('_tss_amax_' + str(s))\n",
    "#     names.append('_tss_sum_' + str(s))\n",
    "\n",
    "# for i, name in enumerate(names):\n",
    "#     adata.varm['embedding' + name] = np.asarray(z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# names = ['', '_m1', '_1', '_amax', '_amax_m1', '_amax_1']\n",
    "\n",
    "# for i, name in enumerate(names):\n",
    "#     adata.varm['embedding' + name] = np.asarray(z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 0\n",
    "for i in range(5):\n",
    "    for j in ['', '_m1', '_1']:\n",
    "        adata.varm['embedding_amax_' + str(i) + str(j)] = np.asarray(z[k])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_3 = '/home/ec2-user/cell_census/tabula_sapiens__trio_top5_pc__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata.write_h5ad(PATH_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AxisArrays with keys: embedding, embedding_m1, embedding_1, embedding_amax, embedding_amax_m1, embedding_amax_1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(adata.varm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_1 = '/home/ec2-user/cell_census/tabula_sapiens__all_pc__sample_donor_id__label_cell_type.h5ad'\n",
    "PATH_2 = '/home/ec2-user/cell_census/tabula_sapiens__all__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata_1 = ad.read_h5ad(PATH_1)\n",
    "adata_2 = ad.read_h5ad(PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_1.varm['embedding_tss_amax'][:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2.varm['embedding_tss_amax'][:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(adata_1.varm['embedding_tss_amax'], adata_2.varm['embedding_tss_amax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(adata_1.varm['embedding'] - adata_2.varm['embedding']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
