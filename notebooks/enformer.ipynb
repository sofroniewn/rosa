{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting fasta file\n",
      "Fasta file done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/enformer-official-rough were not used when initializing Enformer: ['_heads.mouse.0.weight', '_heads.mouse.0.bias']\n",
      "- This IS expected if you are initializing Enformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Enformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Enformer(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): Residual(\n",
       "      (fn): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionPool(\n",
       "      (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "      (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (conv_tower): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crop_final): TargetLengthCrop()\n",
       "  (final_pointwise): Sequential(\n",
       "    (0): Rearrange('b n d -> b d n')\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU()\n",
       "      (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): Rearrange('b d n -> b n d')\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): GELU()\n",
       "  )\n",
       "  (_trunk): Sequential(\n",
       "    (0): Rearrange('b n d -> b d n')\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Rearrange('b d n -> b n d')\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TargetLengthCrop()\n",
       "    (6): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "  )\n",
       "  (_heads): ModuleDict(\n",
       "    (human): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "\n",
    "# PATH = '/home/ec2-user/cell_census/tabula_sapiens__sample_single_cell__label_cell_type__processed.h5ad'\n",
    "PATH = '/home/ec2-user/cell_census/tabula_sapiens__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata = ad.read_h5ad(PATH)\n",
    "\n",
    "from enformer_pytorch import GenomeIntervalDataset\n",
    "\n",
    "class MyGenomeIntervalDataset(GenomeIntervalDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyGenomeIntervalDataset, self).__init__(**kwargs)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        item = super().__getitem__(ind)\n",
    "        label = self.df.row(ind)[4]\n",
    "        return label, item\n",
    "\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import zarr\n",
    "from enformer_pytorch import Enformer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pyfaidx\n",
    "from pathlib import Path\n",
    "\n",
    "torch.multiprocessing.freeze_support()\n",
    "\n",
    "BASE_PT = \"/home/ec2-user/enformer\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# BASE_PT = \"/Users/nsofroniew/Documents/data/multiomics/enformer\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "FASTA_PT = BASE_PT + \"/Homo_sapiens.GRCh38.dna.toplevel.fa\"\n",
    "GENE_INTERVALS_PT = BASE_PT + \"/Homo_sapiens.GRCh38.genes.bed\"\n",
    "GENE_INTERVALS_VAR_PT = BASE_PT + \"/Homo_sapiens.GRCh38.genes.clinvar.vcf.bed\"\n",
    "EMBEDDING_PT = BASE_PT + \"/Homo_sapiens.GRCh38.genes.enformer_embeddings.zarr\"\n",
    "EMBEDDING_PT_TSS = BASE_PT + \"/Homo_sapiens.GRCh38.genes.enformer_embeddings_tss.zarr\"\n",
    "MODEL_PT = \"EleutherAI/enformer-official-rough\"\n",
    "\n",
    "def filter_df_fn(df, name=\"column_5\"):\n",
    "    return df.filter(pl.col(name).is_in(list(adata.var_names)))\n",
    "\n",
    "print(\"Converting fasta file\")\n",
    "pyfaidx.Faidx(FASTA_PT)\n",
    "print(\"Fasta file done\")\n",
    "\n",
    "model = Enformer.from_pretrained(MODEL_PT, output_heads=dict(human = 5313), use_checkpointing = False)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_fn(df, name=\"column_5\"):\n",
    "    return df.filter(pl.col(name).is_in(list(adata.var_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(GENE_INTERVALS_VAR_PT, separator='\\t', has_header=False)\n",
    "df = filter_df_fn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enformer_pytorch.data import str_to_one_hot\n",
    "\n",
    "\n",
    "class MyGenomeIntervalVariantDataset(GenomeIntervalDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyGenomeIntervalVariantDataset, self).__init__(**kwargs)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        item = super().__getitem__(ind)\n",
    "        label = self.df.row(ind)[4]\n",
    "        start = self.df.row(ind)[1]\n",
    "        pos = self.df.row(ind)[5]\n",
    "        ref = self.df.row(ind)[6]\n",
    "        alt = self.df.row(ind)[7]\n",
    "        loc = pos - start - 1\n",
    "        assert abs(item[loc] - str_to_one_hot(ref)).max() == 0.0 # confirm ref match\n",
    "        return label, item, loc, str_to_one_hot(alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143190"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyGenomeIntervalVariantDataset(\n",
    "    bed_file=GENE_INTERVALS_VAR_PT,  # bed file - columns 0, 1, 2 must be <chromosome>, <start position>, <end position>\n",
    "    fasta_file=FASTA_PT,  # path to fasta file\n",
    "    return_seq_indices=False,  # return nucleotide indices (ACGTN) or one hot encodings\n",
    "    rc_aug=False,\n",
    "    filter_df_fn=filter_df_fn,\n",
    ")\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=False, num_workers=0) # type: DataLoader\n",
    "\n",
    "# Create zarr files\n",
    "SEQ_EMBED_DIM = 896\n",
    "EMBED_DIM = 3072\n",
    "NUM_GENES = len(ds)\n",
    "TSS = int(SEQ_EMBED_DIM // 2)\n",
    "\n",
    "paths = (Path(EMBEDDING_PT), Path(EMBEDDING_PT_TSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, seq, pos, alt = ds[100]\n",
    "# seq[pos] = alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# targets_txt = 'https://raw.githubusercontent.com/calico/basenji/0.5/manuscripts/cross2020/targets_human.txt'\n",
    "# df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets = pd.read_csv(BASE_PT + '/targets_human.txt')\n",
    "df_targets.shape  # (5313, 8) With rows match output shape above.\n",
    "cage_indices = np.where(df_targets['description'].str.startswith('CAGE:'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "EMBEDDING_PT_TSS = BASE_PT + \"/Homo_sapiens.GRCh38.genes.enformer_embeddings_inds.zarr\"\n",
    "z = zarr.open(EMBEDDING_PT_TSS)\n",
    "genes = list(adata.var_names)\n",
    "max_inds_dict = dict(zip(genes, list(z[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_embedding_full = zarr.open(\n",
    "#     EMBEDDING_PT,\n",
    "#     mode=\"w\",\n",
    "#     shape=(NUM_GENES, SEQ_EMBED_DIM, EMBED_DIM),\n",
    "#     chunks=(1, SEQ_EMBED_DIM, EMBED_DIM),\n",
    "#     dtype='float32',\n",
    "# )\n",
    "\n",
    "# z_embedding_tss = zarr.open(\n",
    "#     EMBEDDING_PT_TSS,\n",
    "#     mode=\"w\",\n",
    "#     shape=(NUM_GENES, EMBED_DIM),\n",
    "#     chunks=(1, EMBED_DIM),\n",
    "#     dtype='float32',\n",
    "# )\n",
    "\n",
    "# index = 0\n",
    "# for labels, batch in tqdm(dl):\n",
    "#     # calculate embedding\n",
    "#     with torch.no_grad():\n",
    "#         output, embeddings = model(batch.to(DEVICE), return_embeddings=True)\n",
    "#         embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "#     tss_embedding = embeddings[:, TSS]\n",
    "\n",
    "#     # save full and reduced embeddings\n",
    "#     batch_size = len(embeddings)\n",
    "#     z_embedding_full[index : index + batch_size] = embeddings\n",
    "#     z_embedding_tss[index : index + batch_size] = tss_embedding\n",
    "#     index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, seq, pos, alt = ds[0]\n",
    "# seq[pos] = alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ec2-user/rosa/notebooks/enformer.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baws-ec2-1/home/ec2-user/rosa/notebooks/enformer.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m labels, batch, pos, alt \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dl))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "labels, batch, pos, alt = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, embeddings = model(batch.to(DEVICE), return_embeddings=True)\n",
    "    cage_expression = output['human'][:, :, cage_indices].mean(dim=-1)\n",
    "    max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "    batch_size = len(embeddings)\n",
    "    tss_embedding = embeddings[torch.arange(batch_size), max_inds]\n",
    "\n",
    "    batch[torch.arange(batch.shape[0]), pos] = alt.squeeze(dim=1)\n",
    "    output, embeddings = model(batch.to(DEVICE), return_embeddings=True)\n",
    "    tss_embedding_var = embeddings[torch.arange(batch_size), max_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_embedding - tss_embedding_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings[torch.arange(2), :, max_inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_expression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, seq = ds[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, embeddings = model(seq.to(DEVICE), return_embeddings=True)\n",
    "    cage_expression = output['human'][:, cage_indices].mean(dim=1)\n",
    "    max_ind = torch.argmax(cage_expression)\n",
    "    tss_embedding = embeddings[max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.df['column_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = torch.argmax(cage_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['human'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cage_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([TSS, TSS], [0, 100]);\n",
    "plt.plot(output['human'][:, cage_indices].mean(dim=1).detach().cpu());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyensembl import Genome\n",
    "\n",
    "# gtf_file_path = BASE_PT + '/Homo_sapiens.GRCh38.77.gtf'\n",
    "\n",
    "# genome = Genome(reference_name='GRCh38',\n",
    "#     annotation_name='my_genome_features',\n",
    "#     gtf_path_or_url=gtf_file_path)\n",
    "# # genome.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyensembl import EnsemblRelease\n",
    "\n",
    "genome = EnsemblRelease(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = genome.gene_by_id(label)\n",
    "\n",
    "# Observed and predicted gene expression values were obtained by summing up the observed/predicted CAGE read counts\n",
    "# at all unique TSS locations of the gene. For each TSS location, we used the 128-bp bin overlapping the TSS as well\n",
    "# as the two neighboring bins (3 bins in total).\n",
    "\n",
    "# For each gene, look through all transcipts - protein coding / not, and record offsets from gene start\n",
    "gene.transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def get_tss(gene_id, tss=TSS, length=SEQ_EMBED_DIM, sigma=8):\n",
    "    gene = genome.gene_by_id(gene_id)\n",
    "    starts = np.array([tss + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts], dtype=int)\n",
    "    starts = starts[starts>=0]\n",
    "    starts = starts[starts<length]\n",
    "    vector = np.zeros(length)\n",
    "    vector[starts] = 1.0\n",
    "    if sigma is not None:\n",
    "        vector = gaussian_filter1d(vector, sigma)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(get_tss(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tss_locations(genes, tss=TSS):\n",
    "    locations = {}\n",
    "    for gene_id in genes:\n",
    "        gene = genome.gene_by_id(gene_id)\n",
    "        start_diffs = [tss + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts]\n",
    "        locations[gene_id] = start_diffs\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = get_tss_locations(list(ds.df['column_5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = genome.gene_by_id(label)\n",
    "start_diffs = [TSS + np.round((ts.start - gene.start) / 128) for ts in gene.transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in start_diffs:\n",
    "    plt.plot([st, st], [0, 100], color='k');\n",
    "# plt.plot(output['human'][:, cage_indices].mean(dim=1).cpu());\n",
    "plt.ylim([0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss=TSS):\n",
    "    # Embeddings include\n",
    "    #   TSS\n",
    "    #   sum over all\n",
    "    #   argmax over all\n",
    "    #   sum over TSS\n",
    "    #   argmax over TSS\n",
    "    #   sum over TSS sigma 3, 8, 16\n",
    "    #   argmax over TSS sigma 3, 8, 16\n",
    "    batch_size = embeddings.shape[0]\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    tss_emb = embeddings[:, TSS]\n",
    "    sum_emb = embeddings.sum(dim=1)\n",
    "    max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "    amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "    \n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    amax_tss_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "\n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    sum_tss_emb = (embeddings * scaled_cage_expression).sum(dim=1)\n",
    "\n",
    "    all_emb = [tss_emb, sum_emb, amax_emb, amax_tss_emb, sum_tss_emb]\n",
    "    for sigma in sigmas:\n",
    "        ks = 2 * int(sigma / 2 * 3)\n",
    "        tss_tensors_conv = gaussian_filter_1d(tss_tensors, kernel_size=ks, sigma=sigma)\n",
    "        scaled_cage_expression = cage_expression * tss_tensors_conv\n",
    "        max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "        amax_tss_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "\n",
    "        max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "        sum_tss_emb = (embeddings * scaled_cage_expression).sum(dim=1)\n",
    "\n",
    "        all_emb.append(amax_tss_emb)\n",
    "        all_emb.append(sum_tss_emb)\n",
    "\n",
    "    return torch.stack(all_emb, dim=0) # 5 + 2 * len(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss):\n",
    "    # Embeddings include\n",
    "    #   TSS -1, 0, 1\n",
    "    #   argmax over TSS -1, 0, 1\n",
    "    batch_size = embeddings.shape[0]\n",
    "    len_seq = tss_tensors.shape[1] - 1\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    tss_emb = embeddings[:, tss]\n",
    "    tss_emb_m1 = embeddings[:, tss - 1]\n",
    "    tss_emb_1 = embeddings[:, tss + 1]\n",
    "\n",
    "    max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "    amax_emb_m1 = embeddings[torch.arange(batch_size), torch.clip(max_inds - 1, 0, len_seq)]\n",
    "    amax_emb_1 = embeddings[torch.arange(batch_size), torch.clip(max_inds + 1, 0, len_seq)]\n",
    "    \n",
    "    all_emb = [tss_emb, tss_emb_m1, tss_emb_1, amax_emb, amax_emb_m1, amax_emb_1]\n",
    "    return torch.stack(all_emb, dim=0) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embeddings, cage_expression, tss_tensors, sigmas, tss):\n",
    "    # Embeddings include\n",
    "    #   argmax over TSS -1, 0, 1 for top 5\n",
    "    batch_size = embeddings.shape[0]\n",
    "    len_seq = tss_tensors.shape[1] - 1\n",
    "    scaled_cage_expression = cage_expression * tss_tensors\n",
    "\n",
    "    # max_inds = torch.argmax(scaled_cage_expression, dim=-1)\n",
    "    topk_inds, tok_values = torch.topk(scaled_cage_expression, 5, dim=-1)\n",
    "    topk_inds[tok_values == 0] = tss\n",
    "\n",
    "    all_emb = []\n",
    "    for i in range(topk_inds.shape[1]):\n",
    "        max_inds = topk_inds[:, i]\n",
    "        amax_emb = embeddings[torch.arange(batch_size), max_inds]\n",
    "        amax_emb_m1 = embeddings[torch.arange(batch_size), torch.clip(max_inds - 1, 0, len_seq)]\n",
    "        amax_emb_1 = embeddings[torch.arange(batch_size), torch.clip(max_inds + 1, 0, len_seq)]\n",
    "        all_emb += [amax_emb, amax_emb_m1, amax_emb_1]\n",
    "    return torch.stack(all_emb, dim=0) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((2, 89))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(A, 5, dim=-1).indices[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(A, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_cage_expression = cage_expression * tss_tensors\n",
    "max_inds = torch.argmax(cage_expression, dim=-1)\n",
    "tss_embedding = embeddings[torch.arange(batch_size), max_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "\n",
    "PATH = '/home/ec2-user/cell_census/tabula_sapiens__tss_max__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata = ad.read_h5ad(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list(adata.varm.keys()):\n",
    "    del adata.varm[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "z = zarr.open('/home/ec2-user/enformer/Homo_sapiens.GRCh38.genes.enformer_embeddings_5x_top10_pc_0.zarr', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "#     # Embeddings include\n",
    "#     #   TSS\n",
    "#     #   sum over all\n",
    "#     #   argmax over all\n",
    "#     #   argmax over TSS\n",
    "#     #   sum over TSS\n",
    "#     #   argmax over TSS sigma 3, 8, 16\n",
    "#     #   sum over TSS sigma 3, 8, 16\n",
    "\n",
    "# names = ['', '_sum', '_amax', '_tss_amax', '_tss_sum']\n",
    "# sigmas = [3, 8, 16, 32, 64]\n",
    "# for s in sigmas:\n",
    "#     names.append('_tss_amax_' + str(s))\n",
    "#     names.append('_tss_sum_' + str(s))\n",
    "\n",
    "# for i, name in enumerate(names):\n",
    "#     adata.varm['embedding' + name] = np.asarray(z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# names = ['', '_m1', '_1', '_amax', '_amax_m1', '_amax_1']\n",
    "\n",
    "# for i, name in enumerate(names):\n",
    "#     adata.varm['embedding' + name] = np.asarray(z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 0\n",
    "for i in range(10):\n",
    "    for j in ['', '_m2', '_m1', '_1', '_2']:\n",
    "        adata.varm['embedding_amax_' + str(i) + str(j)] = np.asarray(z[k])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_3 = '/home/ec2-user/cell_census/tabula_sapiens__5x_top10_pc__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata.write_h5ad(PATH_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0, :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(adata.varm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_1 = '/home/ec2-user/cell_census/tabula_sapiens__all_pc__sample_donor_id__label_cell_type.h5ad'\n",
    "PATH_2 = '/home/ec2-user/cell_census/tabula_sapiens__all__sample_donor_id__label_cell_type.h5ad'\n",
    "\n",
    "adata_1 = ad.read_h5ad(PATH_1)\n",
    "adata_2 = ad.read_h5ad(PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_1.varm['embedding_tss_amax'][:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_2.varm['embedding_tss_amax'][:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(adata_1.varm['embedding_tss_amax'], adata_2.varm['embedding_tss_amax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(adata_1.varm['embedding'] - adata_2.varm['embedding']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
