{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from rosa.preprocessing import (\n",
    "    clean_cells_genes,\n",
    ")\n",
    "\n",
    "RAW_ADATA_PT = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features.h5ad\"\n",
    "EMBEDS_ADATA_PT = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features_with_embeds_new_norm.h5ad\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(EMBEDS_ADATA_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['counts'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['counts_normalized'] = adata.layers['counts'].copy()\n",
    "sc.pp.normalize_total(adata, target_sum=1e5, layer='counts_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.layers['counts_normalized'].flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosa.preprocessing import bin_expression, reconstruct_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_expression(adata, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_expression(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((adata.X - adata.layers['reconstructed'])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plt.hist((adata.layers['reconstructed'] - adata.X).ravel(), bins=1000);\n",
    "plt.xlim([-.25, .25]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cells and genes not trained on (when possible)\n",
    "adata.layers['prediction'] = adata.layers['reconstructed']\n",
    "test_genes = np.logical_not(adata.var[\"train\"])\n",
    "test_cells = np.logical_not(adata.obs[\"train\"])\n",
    "adata_test = adata[test_cells, test_genes]\n",
    "sc.tl.dendrogram(adata_test, groupby=\"label\", use_rep=\"X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosa.plotting import plot_marker_gene_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_genes_dict = adata_test.obs.set_index('label').to_dict()['marker_feature_name']\n",
    "plot_marker_gene_heatmap(adata_test, marker_genes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.layers['binned'].flatten(), bins=25, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.X.flatten(), bins=250, density=True);\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlim([0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty((10, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class EmbeddingType(Enum):\n",
    "    JOINT = auto()\n",
    "    VAR = auto()\n",
    "    OBS = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingType.JOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(EmbeddingType.__members__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = np.ceil(adata.X)\n",
    "sc.pp.filter_genes(adata, min_cells=1)\n",
    "sc.experimental.pp.normalize_pearson_residuals(adata)\n",
    "adata.X[adata.X<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf(adata.X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(adata.X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = np.ceil(adata.X)\n",
    "adata.layers[\"counts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_cells=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(adata, min_genes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.layers['counts'].sum(axis=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.layers['counts'].sum(axis=1).mean() / 1e5)\n",
    "print(adata.layers['counts'].sum(axis=1).var() / 1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts_normalized_total\"] = adata.X.copy()\n",
    "sc.pp.normalize_total(adata, 1e5, layer=\"counts_normalized_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.layers['counts_normalized_total'].sum(axis=1).mean() / 1e5)\n",
    "print(adata.layers['counts_normalized_total'].sum(axis=1).var() / 1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts_normalized_pearson\"] = adata.X.copy()\n",
    "adata.layers['counts_normalized_pearson'] = np.ceil(adata.layers['counts_normalized_pearson'])\n",
    "sc.experimental.pp.normalize_pearson_residuals(adata, layer=\"counts_normalized_pearson\", theta=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(adata.layers['counts_normalized_pearson']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata.layers['counts_normalized_pearson'].sum(axis=1).mean())\n",
    "print(adata.layers['counts_normalized_pearson'].sum(axis=1).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.layers['counts_normalized_pearson'].flatten(), np.linspace(0, 100, 1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.layers['counts'].flatten(), np.linspace(0, 100, 1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(adata.layers['counts_normalized_pearson'] - adata.layers['counts']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['counts_normalized_pearson'] = np.round(adata.layers['counts_normalized_pearson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(adata.layers['counts_normalized_pearson'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['counts_normalized_pearson'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.uns['pearson_residuals_normalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABULA_SAPIENS_BY_CELL_TYPE_WITH_EMBEDS_PT = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features_with_embeds_new_norm.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_norm = ad.read_h5ad(TABULA_SAPIENS_BY_CELL_TYPE_WITH_EMBEDS_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.exp(adata_norm.X) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_norm.X = adata_norm.layers['counts'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata_norm, 1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abs(adata_norm.X - y)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import kl_div\n",
    "\n",
    "# y_hat = np.asarray(adata[keep_cells].X.flatten())\n",
    "# y = np.asarray(adata[keep_cells].layers['prediction'].flatten())\n",
    "\n",
    "# kl_div(y, y_hat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, poisson\n",
    "\n",
    "y_hat = np.asarray(adata[keep_cells].X.flatten())\n",
    "y = np.asarray(adata[keep_cells].layers['prediction'].flatten())\n",
    "\n",
    "kstest(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(y, 'poisson', args=(np.mean(y),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, poisson\n",
    "\n",
    "poisson_dist = poisson(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = poisson_dist.rvs(size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, _ = np.histogram(y, bins=bins)\n",
    "hist_hat, _ = np.histogram(y_hat, bins=bins)\n",
    "plt.bar(bins[:-1], hist/hist.sum(), width=10/1000)\n",
    "plt.bar(bins[:-1], hist_hat/hist_hat.sum(), alpha=0.5, width=10/1000)\n",
    "plt.ylim([0, 1e-2]);\n",
    "plt.xlim([0, 8]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_dist = poisson(np.mean(y))\n",
    "y_new = poisson_dist.rvs(size=10000)\n",
    "\n",
    "\n",
    "hist, _ = np.histogram(y, bins=bins)\n",
    "hist_new, _ = np.histogram(y_new, bins=bins)\n",
    "plt.bar(bins[:-1], hist/hist.sum(), width=10/1000)\n",
    "plt.bar(bins[:-1], hist_new//hist_new.sum(), alpha=0.5, width=10/1000)\n",
    "plt.ylim([0, 1e-2]);\n",
    "plt.xlim([0, 8]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = [.2, 0.1, 1.2, 0.0.001]\n",
    "\n",
    "result = minimize(negative_binomial, initial_params, args=(y,), method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the optimal parameters\n",
    "r1, p1, r2, p2 = result.x\n",
    "data = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import nbinom\n",
    "\n",
    "r1, p1, r2, p2 = (0.1, 0.1, 1.1, 0.1)\n",
    "\n",
    "# nbinom_dist_1 = nbinom(9.1, 0.6)\n",
    "# nbinom_dist_2 = nbinom(1.2, .001)\n",
    "y_new = (nbinom.rvs(.2, 0.1, size=10000) + nbinom.rvs(1.2, 0.001, size=10000)) / 1000\n",
    "\n",
    "\n",
    "hist, _ = np.histogram(y, bins=bins)\n",
    "hist_new, _ = np.histogram(y_new, bins=bins)\n",
    "plt.bar(bins[:-1], hist/hist.sum(), width=10/1000)\n",
    "plt.bar(bins[:-1], hist_new/hist_new.sum(), alpha=0.5, width=10/1000)\n",
    "plt.ylim([0, 1e-2]);\n",
    "plt.xlim([0, 8]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import nbinom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "data = y\n",
    "\n",
    "# Define the negative binomial function\n",
    "def negative_binomial(params, data):\n",
    "    r1, p1, r2, p2 = params\n",
    "    pmf1 = nbinom.pmf(1000 * data, r1, p1)\n",
    "    pmf2 = nbinom.pmf(1000 * data, r2, p2)\n",
    "    return -np.log(pmf1 + pmf2).sum()\n",
    "\n",
    "# Define the initial values for the parameters\n",
    "initial_params = (0.1, 0.1, 1.1, 0.1)\n",
    "\n",
    "\n",
    "# Minimize the negative binomial function using the Nelder-Mead algorithm\n",
    "result = minimize(negative_binomial, initial_params, args=(data,), method='Nelder-Mead')\n",
    "\n",
    "# Extract the optimal parameters\n",
    "r1, p1, r2, p2 = result.x\n",
    "\n",
    "# Plot the histogram of the data\n",
    "plt.hist(data, bins=30, density=True, alpha=0.5, label='Data')\n",
    "\n",
    "# Plot the sum of the negative binomials\n",
    "x = np.arange(0, data.max())\n",
    "pmf1 = nbinom.pmf(x, r1, p1)\n",
    "pmf2 = nbinom.pmf(x, r2, p2)\n",
    "plt.plot(x, pmf1 + pmf2, 'r-', lw=2, label='Sum of Negative Binomials')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  print(x)\n",
    "except NameError:\n",
    "  print(\"Variable x is not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.3).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm['embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(adata.varm['embedding'].ravel(), bins=2000);\n",
    "plt.xlim([-0.5, 0.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pca on training data\n",
    "pca = PCA()\n",
    "pca.fit(adata.varm['embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = pca.transform(adata.varm['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_));\n",
    "plt.xlim([0, 3042])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)[512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(E[:, :512].ravel(), bins=2000);\n",
    "plt.xlim([-1.5, 1.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "from rosa.preprocessing import (\n",
    "    calculate_gene_embeddings_pca,\n",
    ")\n",
    "\n",
    "\n",
    "EMBEDS_ADATA_PT = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features_with_embeds_new_norm.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(EMBEDS_ADATA_PT)\n",
    "adata = calculate_gene_embeddings_pca(adata, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': 0.918266625236611}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns[\"embedding_pca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(EMBEDS_ADATA_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm['embedding_pca'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.varm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADATA_BULK_PT = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_pbulk.h5ad\"\n",
    "adata = ad.read_h5ad(ADATA_BULK_PT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soma_joinid</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000121410</th>\n",
       "      <td>0</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000268895</th>\n",
       "      <td>1</td>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000118017</th>\n",
       "      <td>10</td>\n",
       "      <td>A4GNT</td>\n",
       "      <td>1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000129968</th>\n",
       "      <td>100</td>\n",
       "      <td>ABHD17A</td>\n",
       "      <td>6163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000236469</th>\n",
       "      <td>1000</td>\n",
       "      <td>AC007040.8</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000256789</th>\n",
       "      <td>9995</td>\n",
       "      <td>RP11-697H9.2</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000255015</th>\n",
       "      <td>9996</td>\n",
       "      <td>RP11-716H6.1</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000255219</th>\n",
       "      <td>9997</td>\n",
       "      <td>RP11-716H6.2</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000255183</th>\n",
       "      <td>9998</td>\n",
       "      <td>LINC02711</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000254528</th>\n",
       "      <td>9999</td>\n",
       "      <td>RP11-728F11.4</td>\n",
       "      <td>1464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57030 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 soma_joinid   feature_name  feature_length\n",
       "feature_id                                                 \n",
       "ENSG00000121410            0           A1BG            3999\n",
       "ENSG00000268895            1       A1BG-AS1            3374\n",
       "ENSG00000118017           10          A4GNT            1779\n",
       "ENSG00000129968          100        ABHD17A            6163\n",
       "ENSG00000236469         1000     AC007040.8             570\n",
       "...                      ...            ...             ...\n",
       "ENSG00000256789         9995   RP11-697H9.2             423\n",
       "ENSG00000255015         9996   RP11-716H6.1             507\n",
       "ENSG00000255219         9997   RP11-716H6.2             571\n",
       "ENSG00000255183         9998      LINC02711             679\n",
       "ENSG00000254528         9999  RP11-728F11.4            1464\n",
       "\n",
       "[57030 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var.set_index(\"feature_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pt = '/home/ec2-user/enformer/Homo_sapiens.GRCh38.genes.enformer_embeddings'\n",
    "var_id = 'ENSG00000280445'\n",
    "full_pt = f'{base_pt}/{var_id}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    var = torch.load(full_pt, map_location='cpu')['embedding']\n",
    "    var = torch.from_numpy(var).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "896 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 896, 3072])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = var.unsqueeze(0).unsqueeze(-3)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = torch.nn.Conv2d(1, 10, (896, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30720])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(a).view(a.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(a.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ToTensor(torch.nn.Module):\n",
    "    \"\"\"Convert ``numpy.ndarray`` to tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype: torch.dtype = torch.float32) -> None:\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, tensor: np.ndarray) -> torch.Tensor:\n",
    "        return torch.from_numpy(tensor).type(torch.float32)\n",
    "\n",
    "class CountNormalize(torch.nn.Module):\n",
    "    \"\"\"Normalize a tensor to a fixed total counts.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_counts=1):\n",
    "        super().__init__()\n",
    "        self.total_counts = total_counts\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.total_counts * F.normalize(tensor, p=1.0, eps=1e-12)\n",
    "\n",
    "\n",
    "class Log1p(torch.nn.Module):\n",
    "    \"\"\"Log1p normalize a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.log1p(tensor)\n",
    "\n",
    "\n",
    "class QuantileNormalize(torch.nn.Module):\n",
    "    \"\"\"Normalize a tensor by quantiles.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins):\n",
    "        super().__init__()\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        boundaries = torch.quantile(tensor, torch.linspace(0, 1, self.n_bins))\n",
    "        return torch.bucketize(tensor, boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionTransform(torch.nn.Sequential):\n",
    "    def __init__(self, cfg):\n",
    "        # Add base transform\n",
    "        transforms = [ToTensor()]\n",
    "\n",
    "        if cfg.total_counts is not None:\n",
    "            transforms.append(CountNormalize(cfg.total_counts))\n",
    "\n",
    "        if cfg.log1p:\n",
    "            transforms.append(Log1p())\n",
    "\n",
    "        if cfg.n_bins is not None:\n",
    "            transforms.append(QuantileNormalize(cfg.n_bins))\n",
    "\n",
    "        super().__init__(*transforms)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExpressionTransformConfig:\n",
    "    total_counts: Optional[int] = None\n",
    "    log1p: Optional[bool] = None\n",
    "    n_bins: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cfg = ExpressionTransformConfig(n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = ExpressionTransform(exp_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3, 1])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(ToTensor()(X).unsqueeze(0), torch.linspace(0, 1, 5), dim=-1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 30])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToTensor()(X).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 1, 5, 9, 7, 2, 0, 1, 2, 7, 6, 9, 4, 9, 6, 3, 8, 8])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(np.random.rand(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Sequential(ToTensor(), CountNormalize(10), Log1p(), QuantileNormalize(10))\n",
    "b = torch.nn.Sequential(ToTensor(), QuantileNormalize(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 30)\n",
    "X [0, :3] = 0\n",
    "X [0, :] = X [0, :] * 1000\n",
    "# X = torch.randint(1, 5, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.04933046e+02,\n",
       "        5.86185246e+02, 3.67209684e+02, 9.28032463e+02, 9.43061788e+02,\n",
       "        6.61347813e+02, 9.09201383e+02, 6.52564336e+01, 7.99837338e+02,\n",
       "        2.61558041e+02, 7.52127084e+02, 9.60963693e+02, 3.92288179e+02,\n",
       "        8.56678094e+02, 6.97483553e+02, 1.11283442e+02, 7.34695066e+02,\n",
       "        8.55954620e+02, 8.99893485e+02, 2.86306631e+02, 1.61621586e+02,\n",
       "        3.46326170e+02, 6.44120733e+02, 9.49631103e+02, 4.06631873e+00,\n",
       "        9.37364444e+02, 9.67401543e+02],\n",
       "       [2.60777832e-01, 9.44569420e-01, 9.31749530e-01, 1.39988490e-01,\n",
       "        5.88439432e-01, 8.94237201e-01, 2.34748228e-01, 1.43530256e-01,\n",
       "        5.14576896e-01, 1.99584205e-01, 5.56390586e-01, 3.41238556e-01,\n",
       "        6.24677666e-01, 4.59784231e-01, 5.80895454e-01, 4.41532664e-01,\n",
       "        5.67134862e-02, 1.51184503e-01, 7.05252488e-01, 6.01157209e-01,\n",
       "        9.84628333e-01, 4.45089557e-01, 2.13056719e-01, 4.27250153e-01,\n",
       "        6.44899373e-01, 2.14565691e-01, 2.20508775e-01, 9.98951048e-02,\n",
       "        9.41615457e-01, 6.41620978e-01],\n",
       "       [8.94089945e-01, 1.12421102e-01, 9.11242744e-01, 5.36732599e-01,\n",
       "        7.66255925e-01, 4.53289727e-01, 9.51083579e-01, 3.75815038e-01,\n",
       "        5.23343990e-01, 8.27400068e-01, 2.82642832e-01, 5.81115355e-01,\n",
       "        5.37591246e-01, 1.85946015e-01, 1.12525304e-01, 3.25400331e-01,\n",
       "        7.10503208e-01, 6.21157848e-01, 2.73205471e-01, 5.35723475e-01,\n",
       "        4.09939801e-01, 7.92092881e-01, 8.55056225e-01, 6.73185017e-01,\n",
       "        6.72154957e-01, 2.06790547e-01, 3.03884416e-01, 1.17774024e-01,\n",
       "        3.79656400e-01, 5.36711374e-01]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 4, 3, 6, 7, 8, 6, 3, 5, 1, 5, 9, 8, 7, 2, 9, 2, 2, 6, 5, 4, 7,\n",
       "         8, 5, 4, 2, 9, 9],\n",
       "        [7, 6, 2, 1, 5, 1, 4, 4, 6, 5, 8, 6, 7, 9, 9, 3, 3, 5, 1, 1, 5, 8, 3, 9,\n",
       "         9, 3, 3, 2, 6, 9],\n",
       "        [7, 2, 1, 4, 8, 4, 3, 8, 2, 7, 2, 6, 9, 7, 8, 7, 5, 6, 4, 5, 8, 3, 8, 6,\n",
       "         1, 2, 7, 3, 4, 4]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 4, 3, 6, 6, 8, 5, 3, 5, 1, 5, 8, 7, 7, 2, 9, 2, 2, 6, 5, 4, 7,\n",
       "         8, 5, 4, 2, 8, 9],\n",
       "        [7, 6, 2, 1, 5, 1, 4, 4, 6, 6, 8, 6, 8, 9, 9, 3, 3, 5, 1, 1, 5, 8, 3, 9,\n",
       "         9, 3, 3, 2, 6, 9],\n",
       "        [7, 2, 1, 4, 9, 4, 3, 8, 2, 7, 2, 6, 9, 7, 9, 7, 5, 7, 4, 5, 8, 3, 8, 6,\n",
       "         1, 2, 7, 3, 4, 4]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([146, 150, 151])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(X).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.24"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20 * 512 / 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.dtype"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unravel_index(10, (9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDS_ADATA_PT = '/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features_with_embeds_new_norm.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDS_ADATA_PT_2 = \"/Users/nsofroniew/Documents/data/multiomics/cell_census/tabula_sapiens_by_features_raw.h5ad\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(EMBEDS_ADATA_PT)\n",
    "adata_2 = ad.read_h5ad(EMBEDS_ADATA_PT_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032808773"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers['counts'][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91241.07"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_2.X.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10028871"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers['binned'][0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032808773"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_2.X[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ExpressionActivations(Enum):\n",
    "    SOFTPLUS = auto()\n",
    "    SOFTMAX = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ExpressionActivations.SOFTPLUS'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ExpressionActivations.SOFTPLUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softplus'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpressionActivations.SOFTPLUS.name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (1): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.append(nn.Linear(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body(torch.rand(10, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x287ac0430>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map((body, body), (torch.rand(10, 10), torch.rand(10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() received an invalid combination of arguments - got (tuple), but expected (Tensor input, Tensor other, *, Number alpha, Tensor out)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49madd((torch\u001b[39m.\u001b[39;49mrand(\u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m), torch\u001b[39m.\u001b[39;49mrand(\u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m)))\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: add() received an invalid combination of arguments - got (tuple), but expected (Tensor input, Tensor other, *, Number alpha, Tensor out)"
     ]
    }
   ],
   "source": [
    "torch.add((torch.rand(10, 10), torch.rand(10, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.rand(10, 10), torch.rand(10, 10)), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class AttentionEmbeds(nn.Module):\n",
    "    def __init__(self, in_dim: Tuple[int, int], out_dim) -> None:\n",
    "        super(AttentionEmbeds, self).__init__()\n",
    "        \n",
    "        self.value = nn.Parameter(torch.randn(out_dim))\n",
    "        self.activation = nn.GELU()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, x: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        atten = self.activation(torch.einsum('...i, ...i ->...', *x))\n",
    "        return torch.einsum('..., i -> ...i', atten, self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.rand((10, 20))\n",
    "x_2 = torch.rand((10, 20))\n",
    "v = torch.rand(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = torch.einsum('...i, ...i ->...', x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.einsum('..., i -> ...i', atten, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.9391, -2.6665,  1.7923,  0.1971,  0.0085, -0.5108,  1.6944, -0.3136,\n",
       "        -0.0164, -0.9281,  1.5383,  0.5811,  0.5446, -0.3123, -0.4335, -0.1784,\n",
       "         1.1701, -0.0980,  0.4092,  1.2190], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.randn(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AttentionEmbeds((20, 20), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a((x_1, x_2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 896, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = '/Users/nsofroniew/Documents/data/multiomics/enformer/scratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = zarr.open(PT + '/example.zarr', mode='w', shape=(1000, 896, 3072), chunks=(1, None, None), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    z1[i * 100: (i+1)*100] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = zarr.open(PT + '/example.zarr', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004496097564697266\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "z2[893]\n",
    "stop = time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'results': X[0]}, PT + '/example_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09557294845581055\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "torch.load(PT + '/example_0.pt')\n",
    "stop = time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarrDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        super(ZarrDataset, self).__init__()\n",
    "\n",
    "        self.path = path\n",
    "\n",
    "        # self.array = zarr.open(path, mode='r')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return 3 #self.array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ZarrDataset(PT + '/example.zarr')\n",
    "dl = torch.utils.data.DataLoader(ds, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)  File \"/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "\n",
      "  File \"/Users/nsofroniew/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    AttributeErrorself = reduction.pickle.load(from_parent)\n",
      ": Can't get attribute 'ZarrDataset' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'ZarrDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 29075) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1128\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1129\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 29075) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[39m=\u001b[39m time()\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m(dl):\n\u001b[1;32m      3\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m      4\u001b[0m stop \u001b[39m=\u001b[39m time()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1324\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1323\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1324\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1327\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1290\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1290\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1291\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1292\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/multiomics/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1141\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 29075) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for batch in iter(dl):\n",
    "    pass\n",
    "stop = time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
